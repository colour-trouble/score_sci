{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yang-song/score_sde_pytorch/blob/main/Score_SDE_demo_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBJappt3toqj"
      },
      "source": [
        "# Getting started\n",
        "\n",
        "1. Setting up conda environtment\n",
        "\n",
        "    ```\n",
        "    conda env create --name envname --file=environment.yml\n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa9OIcJmUKmZ",
        "outputId": "8fde468b-2c95-4003-f0bd-20ddad5248e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zhenyuen/miniconda3/envs/pytorch-dummy/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "#@title Autoload all modules\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import torch\n",
        "import sampling\n",
        "import datasets\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import models\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from losses import get_optimizer\n",
        "from models.ema import ExponentialMovingAverage\n",
        "from models import utils as mutils\n",
        "from models import ncsnpp\n",
        "from utils import restore_checkpoint\n",
        "from sde_lib import VESDE\n",
        "from sampling import (ReverseDiffusionPredictor,\n",
        "                      LangevinCorrector,)\n",
        "\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "-reedYgCU79v"
      },
      "outputs": [],
      "source": [
        "# @title Score model setup (only VESDE for now)\n",
        "\n",
        "sde = 'vesde'\n",
        "dataset_id = 'FFHQ'  # Dataset here corresponds to that used for training\n",
        "# dataset_id = 'CIFAR10'\n",
        "\n",
        "sequence_len = 8  # Represents the number of frames in the video sequence\n",
        "iterations = 2000 # Number of sampling iterations == no. of noise scales\n",
        "\n",
        "if sde.lower() == 'vesde':\n",
        "    if dataset_id.lower() == 'cifar10':\n",
        "        # from configs.vp import cifar10_ddpmpp_continuous as configs\n",
        "        # ckpt_filename = \"/home/zhenyuen/Documents/model_checkpoints/vp/ddpm/cifar10_ddpmpp_continuous/checkpoint_26.pth\"\n",
        "        from configs.ve import cifar10_ncsnpp_continuous as configs\n",
        "        ckpt_filename = \"./checkpoints/ve/cifar10_ncsnpp_continuous/checkpoint_24.pth\"\n",
        "    elif dataset_id.lower() == 'ffhq':\n",
        "        from configs.ve import ffhq_256_ncsnpp_continuous as configs\n",
        "        ckpt_filename = \"./checkpoints/ve/ffhq_256_ncsnpp_continuous/checkpoint_48.pth\"\n",
        "\n",
        "    config = configs.get_config()\n",
        "    config.training.batch_size = sequence_len  # Required by model\n",
        "    config.eval.batch_size = sequence_len  # Required by model\n",
        "    config.sequence_len = sequence_len\n",
        "    config.model.num_scales = iterations # Overwrite no. iterations here\n",
        "\n",
        "    sde = VESDE(sigma_min=config.model.sigma_min,\n",
        "                sigma_max=config.model.sigma_max, N=config.model.num_scales)\n",
        "    \n",
        "    sampling_eps = 1e-5\n",
        "\n",
        "elif sde.lower() == 'vpsde':\n",
        "    pass\n",
        "\n",
        "elif sde.lower() == 'subvpsde':\n",
        "    pass\n",
        "\n",
        "\n",
        "random_seed = 0  # @param {\"type\": \"integer\"}\n",
        "\n",
        "sigmas = mutils.get_sigmas(config)\n",
        "scaler = datasets.get_data_scaler(config)\n",
        "inverse_scaler = datasets.get_data_inverse_scaler(config)\n",
        "score_model = mutils.create_model(config)\n",
        "\n",
        "optimizer = get_optimizer(config, score_model.parameters())\n",
        "ema = ExponentialMovingAverage(score_model.parameters(),\n",
        "                               decay=config.model.ema_rate)\n",
        "state = dict(step=0, optimizer=optimizer,\n",
        "             model=score_model, ema=ema)\n",
        "\n",
        "state = restore_checkpoint(ckpt_filename, state, config.device)\n",
        "ema.copy_to(score_model.parameters())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "G8ei2Xsfg6JQ"
      },
      "outputs": [],
      "source": [
        "# @title Visualization\n",
        "\n",
        "def image_grid(x):\n",
        "    size = config.data.image_size\n",
        "    channels = config.data.num_channels\n",
        "    img = x.reshape(-1, size, size, channels)\n",
        "    w = int(np.sqrt(img.shape[0]))\n",
        "    img = img.reshape((w, w, size, size, channels)).transpose(\n",
        "        (0, 2, 1, 3, 4)).reshape((w * size, w * size, channels))\n",
        "    return img\n",
        "\n",
        "\n",
        "def show_samples(gen, ori, meas, dir_name=None):\n",
        "    gen = gen.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
        "    ori = ori.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
        "    stack = []\n",
        "    # Number of images per row is hard coded to 4, change if necessary\n",
        "    stack.append(ori[0:4])\n",
        "    # This is done as all testing samples thus far are 8-frame video sequences.\n",
        "    stack.append(gen[0:4])\n",
        "    stack.append(ori[4:8])\n",
        "    stack.append(gen[4:8])\n",
        "    x = np.concatenate(stack, axis=0)\n",
        "    img = image_grid(x)\n",
        "\n",
        "    if dir_name:\n",
        "        # Save measured sample\n",
        "        if not os.path.exists(f\"assets/\"):\n",
        "            os.mkdir(\"assets/\")\n",
        "            \n",
        "        if not os.path.exists(f\"assets/{dir_name}\"):\n",
        "            os.mkdir(f\"assets/{dir_name}\")\n",
        "\n",
        "        meas = meas.permute(1, 2, 0).detach().cpu().numpy()\n",
        "        plt.imshow(meas / 8)\n",
        "        plt.savefig(f\"assets/{dir_name}/measured.png\")\n",
        "\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)\n",
        "\n",
        "    if dir_name:\n",
        "        plt.savefig(f\"assets/{dir_name}/generated.png\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Load original and measured video sequence from dataset\n",
        "\n",
        "def load_dataset(scene, config):\n",
        "    img_size = config.data.image_size\n",
        "    channels = config.data.num_channels\n",
        "    sequence_len = config.sequence_len\n",
        "    shape = (sequence_len, channels, img_size, img_size)\n",
        "\n",
        "    def preprocess_fn(f):\n",
        "        def process(img):\n",
        "            img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "            img = tf.image.resize(\n",
        "                img, (config.data.image_size, config.data.image_size), antialias=True)\n",
        "            img = tf.transpose(img, (2, 0, 1))\n",
        "            return img\n",
        "\n",
        "        def parse(f):\n",
        "            img = tf.io.read_file(f)  # parse\n",
        "            img = tf.image.decode_png(img, channels=3)  # 3 For colour\n",
        "            img = process(img)\n",
        "            return img\n",
        "\n",
        "        return parse(f)\n",
        "\n",
        "    def get_images():\n",
        "        ds = tf.data.Dataset.list_files(\n",
        "            f'./datasets/davis/{scene}/*', shuffle=False)\n",
        "        ds = ds.map(preprocess_fn)\n",
        "        ori = [img for img in ds.take(8)]\n",
        "        ori = tf.stack(ori, axis=0)\n",
        "        assert ori.shape == (8, 3, config.data.image_size,\n",
        "                             config.data.image_size)\n",
        "        return ori.numpy()\n",
        "\n",
        "    def get_rgb_mask():\n",
        "        # path of the .mat data filefile = sio.loadmat(matfile, appendmat=True) # for '-v7.2' and lower version of .mat file (MATLAB)\n",
        "        matfile = './datasets/cacti/crash32_cacti.mat'\n",
        "        # for '-v7.2' and lower version of .mat file (MATLAB)\n",
        "        file = sio.loadmat(matfile, appendmat=True)\n",
        "        mask = np.float32(file['mask'])\n",
        "        mask = mask[None, ...]\n",
        "        mask = np.repeat(mask, 3, 0)\n",
        "        mask = np.transpose(mask, [3, 0, 1, 2])\n",
        "        mask = mask[:, :, 0:config.data.image_size, 0:config.data.image_size]\n",
        "        assert mask.shape == (8, 3, config.data.image_size,\n",
        "                              config.data.image_size)\n",
        "        return mask\n",
        "\n",
        "    def get_meas(ori, mask):\n",
        "        meas = np.zeros(\n",
        "            shape=(3, config.data.image_size, config.data.image_size))\n",
        "        for i in range(ori.shape[0]):\n",
        "            meas += mask[i, :, :, :] * ori[i, :, :, :]\n",
        "        return meas\n",
        "\n",
        "    ori = get_images()\n",
        "    mask = get_rgb_mask()\n",
        "    meas = get_meas(ori, mask)\n",
        "\n",
        "    meas = torch.from_numpy(meas).to(device=config.device)\n",
        "    mask = torch.from_numpy(mask).to(device=config.device)\n",
        "    ori = torch.from_numpy(ori).to(device=config.device)\n",
        "\n",
        "    return (meas, mask, ori)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Setup sampling function\n",
        "def get_sampler(sampler, config):\n",
        "    # Only this predictor/corrector pair tested thus far\n",
        "    predictor = ReverseDiffusionPredictor\n",
        "    corrector = LangevinCorrector\n",
        "\n",
        "    snr = 0.16  # @param {\"type\": \"number\"}\n",
        "    n_steps = 1  # @param {\"type\": \"integer\"}\n",
        "    probability_flow = False  # @param {\"type\": \"boolean\"}\n",
        "\n",
        "    img_size = config.data.image_size\n",
        "    channels = config.data.num_channels\n",
        "    sequence_len = config.sequence_len\n",
        "    shape = (sequence_len, channels, img_size, img_size)\n",
        "\n",
        "    if sampler.lower() == 'cond_rev_sde':\n",
        "        sampling_fn = sampling.get_cond_rev_sde(sde, shape, predictor, corrector,\n",
        "                                                inverse_scaler, snr, n_steps=n_steps,\n",
        "                                                probability_flow=probability_flow,\n",
        "                                                continuous=config.training.continuous,\n",
        "                                                eps=sampling_eps, device=config.device)\n",
        "\n",
        "    elif sampler.lower() == 'prox_op':\n",
        "        coeff = 0.9 # Balancing hyper-parameter\n",
        "        sampling_fn = sampling.get_prox_op_sampler(sde, shape, predictor, corrector,\n",
        "                                                inverse_scaler, snr, n_steps=n_steps,\n",
        "                                                probability_flow=probability_flow,\n",
        "                                                continuous=config.training.continuous,\n",
        "                                                eps=sampling_eps, device=config.device, coeff=coeff)\n",
        "\n",
        "    elif sampler.lower() == 'proj_exp':\n",
        "        # Projecting expectation method -- under Section 5.4 \"Other variations\"\n",
        "        coeff = 0.9 # Balancing hyper-parameter\n",
        "        sampling_fn = sampling.get_proj_exp_sampler(sde, shape, predictor, corrector,\n",
        "                                                inverse_scaler, snr, n_steps=n_steps,\n",
        "                                                probability_flow=probability_flow,\n",
        "                                                continuous=config.training.continuous,\n",
        "                                                eps=sampling_eps, device=config.device, coeff=coeff)        \n",
        "\n",
        "    elif sampler.lower() == 'vid_frame_approx':\n",
        "        coeff = 0.9 # Balancing hyper-parameter\n",
        "        sampling_fn = sampling.get_vid_frame_approx_sampler(sde, shape, predictor, corrector,\n",
        "                                                inverse_scaler, snr, n_steps=n_steps,\n",
        "                                                probability_flow=probability_flow,\n",
        "                                                continuous=config.training.continuous,\n",
        "                                                eps=sampling_eps, device=config.device, coeff=coeff)\n",
        "\n",
        "    return sampling_fn\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start:\n",
            "RGB PSNR at 10: -53.45 dB, Time =  6.112 s\n",
            "RGB PSNR at 20: -53.03 dB, Time =  10.313 s\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/zhenyuen/Documents/Score_SCI/score_sci/demo.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B147.8.89.62/home/zhenyuen/Documents/Score_SCI/score_sci/demo.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m sampling_fn \u001b[39m=\u001b[39m get_sampler(sampler, config)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B147.8.89.62/home/zhenyuen/Documents/Score_SCI/score_sci/demo.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m meas, mask, ori \u001b[39m=\u001b[39m load_dataset(scene, config)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B147.8.89.62/home/zhenyuen/Documents/Score_SCI/score_sci/demo.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m sample \u001b[39m=\u001b[39m sampling_fn(score_model, x\u001b[39m=\u001b[39;49mori, y\u001b[39m=\u001b[39;49mmeas, mask\u001b[39m=\u001b[39;49mmask)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B147.8.89.62/home/zhenyuen/Documents/Score_SCI/score_sci/demo.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m show_samples(sample, ori, meas, dir_name\u001b[39m=\u001b[39mscene)\n",
            "File \u001b[0;32m~/Documents/Score_SCI/score_sci/sampling.py:900\u001b[0m, in \u001b[0;36mget_cond_rev_sde.<locals>.pc_sampler\u001b[0;34m(model, x, y, mask)\u001b[0m\n\u001b[1;32m    897\u001b[0m y_mean, std \u001b[39m=\u001b[39m sde\u001b[39m.\u001b[39mmarginal_prob(y, timesteps[i])\n\u001b[1;32m    899\u001b[0m score_likelihood \u001b[39m=\u001b[39m likelihood(u, y_mean, mask, mask_sum \u001b[39m*\u001b[39m std)\n\u001b[0;32m--> 900\u001b[0m u, u_mean \u001b[39m=\u001b[39m corrector_update_fn(\n\u001b[1;32m    901\u001b[0m     x\u001b[39m=\u001b[39;49mu, t\u001b[39m=\u001b[39;49mvec_t, model\u001b[39m=\u001b[39;49mmodel\n\u001b[1;32m    902\u001b[0m )\n\u001b[1;32m    903\u001b[0m u, u_mean \u001b[39m=\u001b[39m predictor_update_fn(\n\u001b[1;32m    904\u001b[0m     x\u001b[39m=\u001b[39mu, t\u001b[39m=\u001b[39mvec_t, model\u001b[39m=\u001b[39mmodel\n\u001b[1;32m    905\u001b[0m )\n\u001b[1;32m    906\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m9\u001b[39m:\n",
            "File \u001b[0;32m~/Documents/Score_SCI/score_sci/sampling.py:385\u001b[0m, in \u001b[0;36mshared_corrector_update_fn\u001b[0;34m(x, t, sde, model, corrector, continuous, snr, n_steps, likelihood)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     corrector_obj \u001b[39m=\u001b[39m corrector(sde, score_fn, snr, n_steps)\n\u001b[0;32m--> 385\u001b[0m \u001b[39mreturn\u001b[39;00m corrector_obj\u001b[39m.\u001b[39;49mupdate_fn(x, t, likelihood)\n",
            "File \u001b[0;32m~/Documents/Score_SCI/score_sci/sampling.py:300\u001b[0m, in \u001b[0;36mLangevinCorrector.update_fn\u001b[0;34m(self, x, t, likelihood)\u001b[0m\n\u001b[1;32m    298\u001b[0m     grad \u001b[39m=\u001b[39m score_fn(x, t) \u001b[39m+\u001b[39m likelihood\n\u001b[1;32m    299\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m     grad \u001b[39m=\u001b[39m score_fn(x, t)\n\u001b[1;32m    301\u001b[0m noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn_like(x)\n\u001b[1;32m    302\u001b[0m grad_norm \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnorm(grad\u001b[39m.\u001b[39mreshape(grad\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mmean()\n",
            "File \u001b[0;32m~/Documents/Score_SCI/score_sci/models/utils.py:183\u001b[0m, in \u001b[0;36mget_score_fn.<locals>.score_fn\u001b[0;34m(x, t)\u001b[0m\n\u001b[1;32m    180\u001b[0m     labels \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m sde\u001b[39m.\u001b[39mN \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    181\u001b[0m     labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mround(labels)\u001b[39m.\u001b[39mlong()\n\u001b[0;32m--> 183\u001b[0m score \u001b[39m=\u001b[39m model_fn(x, labels)\n\u001b[1;32m    184\u001b[0m \u001b[39mreturn\u001b[39;00m score\n",
            "File \u001b[0;32m~/Documents/Score_SCI/score_sci/models/utils.py:130\u001b[0m, in \u001b[0;36mget_model_fn.<locals>.model_fn\u001b[0;34m(x, labels)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m train:\n\u001b[1;32m    129\u001b[0m     model\u001b[39m.\u001b[39meval()\n\u001b[0;32m--> 130\u001b[0m     \u001b[39mreturn\u001b[39;00m model(x, labels)\n\u001b[1;32m    131\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-dummy/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-dummy/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:168\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[1;32m    167\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 168\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-dummy/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:178\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_apply\u001b[39m(\u001b[39mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 178\u001b[0m     \u001b[39mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(replicas)])\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-dummy/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py:78\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     76\u001b[0m         thread\u001b[39m.\u001b[39mstart()\n\u001b[1;32m     77\u001b[0m     \u001b[39mfor\u001b[39;00m thread \u001b[39min\u001b[39;00m threads:\n\u001b[0;32m---> 78\u001b[0m         thread\u001b[39m.\u001b[39;49mjoin()\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     _worker(\u001b[39m0\u001b[39m, modules[\u001b[39m0\u001b[39m], inputs[\u001b[39m0\u001b[39m], kwargs_tup[\u001b[39m0\u001b[39m], devices[\u001b[39m0\u001b[39m])\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-dummy/lib/python3.9/threading.py:1053\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1053\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[1;32m   1054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1055\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-dummy/lib/python3.9/threading.py:1073\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1073\u001b[0m     \u001b[39mif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[1;32m   1074\u001b[0m         lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m   1075\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Select scene to reconstruct under /datasets/davis/\n",
        "# scene = 'walking'\n",
        "scene = 'bear'\n",
        "# scene = 'boat'\n",
        "# scene = 'dog'\n",
        "# scene = 'train'\n",
        "\n",
        "# Select sampler\n",
        "sampler = 'cond_rev_sde'  # Conditional reverse-time SDE method\n",
        "# sampler = 'prox_op' # Proximal optimization method\n",
        "# sampler = 'proj_exp' # Projecting expectation method\n",
        "# sampler = 'vid_frame_approx' # Video frame approximation method\n",
        "\n",
        "\n",
        "sampling_fn = get_sampler(sampler, config)\n",
        "meas, mask, ori = load_dataset(scene, config)\n",
        "\n",
        "sample = sampling_fn(score_model, x=ori, y=meas, mask=mask)\n",
        "\n",
        "# Pass dir_name argument to save sample under assets/dir_name\n",
        "dir_name = f\"{scene}_{sampler}\"\n",
        "show_samples(sample, ori, meas, dir_name=dir_name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Score SDE demo PyTorch",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('pytorch-dummy')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "93fe6f1ab4fd72745e4265a6e0a0290e7a4617d1f4c2d9861ab3e69aff27da8a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
